{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook for experimenting with modalities data, written in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TCN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple TCN model to embed sequentail data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is of size (N, C, L) where N is the batch size, C is the number of features, L is the sequence length\n",
    "        y1 = self.tcn(x)  # input should have dimension (N, C, L)\n",
    "        o = self.linear(y1[:, :, -1])\n",
    "        return o\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    '''\n",
    "    A TemporalConvNet is a stack of TemporalBlock modules, each of which has a dilated convolutional layer.\n",
    "    '''\n",
    "    def __init__(self, input_size, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = input_size if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size, padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "class TemporalBlock(nn.Module):\n",
    "    '''\n",
    "    A temporal block is a stack of two dilated causal convolutional layers with the same dilation factor.\n",
    "    '''\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    '''\n",
    "    This layer removes the padding from the right side of the input.\n",
    "    '''\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n",
      "torch.Size([10, 44, 100])\n"
     ]
    }
   ],
   "source": [
    "# test TCN module\n",
    "\n",
    "x = torch.randn(10, 16, 100) # batch size 10, 16 features, 100 time steps\n",
    "model = TCN(16, 10, [25]*8, 2, 0.2) # 16 input features, 10 output classes, 8 layers, 25 channels per layer, kernel size 2, dropout 0.2\n",
    "print(model(x).shape) # should be (10, 10)\n",
    "\n",
    "# test TemporalConvNet module\n",
    "\n",
    "x = torch.randn(10, 16, 100) # batch size 10, 16 features, 100 time steps\n",
    "model = TemporalConvNet(16, [44]*8, 2, 0.2) # 16 input features, 8 layers, 44 channels per layer, kernel size 2, dropout 0.2\n",
    "print(model(x).shape) # should be (10, 44, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Encoder with TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageTCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(ImageTCN, self).__init__()\n",
    "        self.tcn = ImageTemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is of size (N, C, H, W) where N is the batch size, C is the number of features, H is the height, W is the width\n",
    "        y1 = self.tcn(x)  # input should have dimension (N, C, H, W)\n",
    "        o = self.linear(y1[:, :, -1, -1])\n",
    "        return F.log_softmax(o, dim=1)\n",
    "    \n",
    "class ImageTemporalConvNet(nn.Module):\n",
    "    '''\n",
    "    An ImageTemporalConvNet is a stack of ImageTemporalBlock modules, each of which has a dilated convolutional layer.\n",
    "    '''\n",
    "    def __init__(self, input_size, num_channels, kernel_size=3, dropout=0.2):\n",
    "        super(ImageTemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = input_size if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [ImageTemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size, padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "class ImageTemporalBlock(nn.Module):\n",
    "    '''\n",
    "    An ImageTemporalBlock is a stack of two dilated causal convolutional layers with the same dilation factor.\n",
    "    '''\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(ImageTemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp1 = ImageChomp2d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(n_outputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp2 = ImageChomp2d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv2d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "    \n",
    "class ImageChomp2d(nn.Module):\n",
    "    '''\n",
    "    This layer removes the padding from the right side of the input.\n",
    "    '''\n",
    "    def __init__(self, chomp_size):\n",
    "        super(ImageChomp2d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size, :-self.chomp_size].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n",
      "torch.Size([10, 44, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# test ImageTCN module\n",
    "\n",
    "x = torch.randn(10, 3, 32, 32) # batch size 10, 3 channels, 32x32 image\n",
    "model = ImageTCN(3, 10, [44]*8, 3, 0.2) # 3 input channels, 10 output classes, 8 layers, 25 channels per layer, kernel size 3, dropout 0.2\n",
    "print(model(x).shape) # should be (10, 10)\n",
    "\n",
    "# test ImageTemporalConvNet module\n",
    "\n",
    "x = torch.randn(10, 3, 32, 32) # batch size 10, 3 channels, 32x32 image\n",
    "model = ImageTemporalConvNet(3, [44]*8, 3, 0.2) # 3 input channels, 8 layers, 44 channels per layer, kernel size 3, dropout 0.2\n",
    "print(model(x).shape) # should be (10, 44, 32, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encode Image with pretrained ResNet18, get the feature map before the last layer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchvision.models.resnet import ResNet18_Weights\n",
    "\n",
    "class ResNet18_Encoder(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ResNet18_Encoder, self).__init__()\n",
    "        if pretrained:\n",
    "            self.resnet18 = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        else:\n",
    "            self.resnet18 = models.resnet18(weights=None)\n",
    "        self.resnet18.fc = nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet18(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512])\n"
     ]
    }
   ],
   "source": [
    "# test ResNet18_Encoder module\n",
    "\n",
    "x = torch.randn(10, 3, 224, 224) # batch size 10, 3 channels, 224x224 image\n",
    "model = ResNet18_Encoder(pretrained=True)\n",
    "print(model(x).shape) # should be (10, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine ResNet18_Encoder and TCN to have video encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Video Temporal Convolutional Network (VTCN) with ResNet18 Encoder\n",
    "\n",
    "class VTCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(VTCN, self).__init__()\n",
    "        self.encoder = ResNet18_Encoder(pretrained=True)\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is of size (N, C, T, H, W) where N is the batch size, C is the number of features, T is the number of frames, H is the height, W is the width\n",
    "        N, C, T, H, W = x.size()\n",
    "        x = x.view(N*T, C, H, W)\n",
    "        y1 = self.encoder(x)  # input should have dimension (N*T, C, H, W)\n",
    "        y1 = y1.view(N, T, -1)\n",
    "        y1 = y1.permute(0, 2, 1) # permute to (N, C, T)\n",
    "        y1 = self.tcn(y1)  # input should have dimension (N, C, T)\n",
    "        o = self.linear(y1[:, :, -1])\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 44])\n"
     ]
    }
   ],
   "source": [
    "# test VTCN module\n",
    "\n",
    "x = torch.randn(10, 3, 16, 224, 224) # batch size 10, 3 channels, 16 frames, 224x224 image\n",
    "model = VTCN(512, 44, [25]*8, 2, 0.2) # 512 input features, 44 output features, 8 layers, 25 channels per layer, kernel size 2, dropout 0.2\n",
    "print(model(x).shape) # should be (10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video embedding vector size torch.Size([10, 44])\n",
      "audio embedding vector size torch.Size([10, 44])\n",
      "egg embedding vector size torch.Size([10, 44])\n"
     ]
    }
   ],
   "source": [
    "# create 3 modalities of data (video, audio, egg) and encode them with either TCN or VTCN\n",
    "\n",
    "# batch size 10, 3 channels, 60 frames, 224x224 image\n",
    "x_video = torch.randn(10, 3, 60, 224, 224)\n",
    "# batch size 10, 1 channel, 16k audio samples\n",
    "x_audio = torch.randn(10, 1, 16000) \n",
    "# batch size 10, 32 channels, 240Hz in 1 second\n",
    "x_egg = torch.randn(10, 32, 240)\n",
    "\n",
    "# Ecoding video with VTCN\n",
    "model_video = VTCN(512, 44, [25]*8, 2, 0.2)\n",
    "output_video = model_video(x_video)\n",
    "print(\"video embedding vector size\", output_video.shape) # should be (10, 44)\n",
    "\n",
    "# Ecoding audio with TCN\n",
    "model_audio = TCN(1, 44, [25]*8, 2, 0.2) # 1 input channel, 44 output classes, 8 layers, 25 channels per layer, kernel size 2, dropout 0.2\n",
    "output_audio = model_audio(x_audio)\n",
    "print(\"audio embedding vector size\", output_audio.shape) # should be (10, 44)\n",
    "\n",
    "# Ecoding egg with TCN\n",
    "model_egg = TCN(32, 44, [25]*8, 2, 0.2)\n",
    "output_egg = model_egg(x_egg)\n",
    "print(\"egg embedding vector size\", output_egg.shape) # should be (10, 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video embedding vector size torch.Size([10, 44])\n",
      "audio embedding vector size torch.Size([10, 44])\n",
      "egg embedding vector size torch.Size([10, 44])\n"
     ]
    }
   ],
   "source": [
    "# create 3 modalities of data (video, audio, egg) and encode them with either ImageTCN or VTCN\n",
    "\n",
    "# batch size 10, 3 channels, 60 frames, 224x224 image\n",
    "x_video = torch.randn(10, 3, 60, 224, 224)\n",
    "# audio spectrogram of size 10, 1 channel, 500 frames, 20 frequency bins\n",
    "x_audio = torch.randn(10, 1, 500, 20)\n",
    "# eeg spectrogram of size 10, 32 channels, 500 frames, 20 frequency bins\n",
    "x_egg = torch.randn(10, 32, 500, 20)\n",
    "\n",
    "# Ecoding video wit VTCN\n",
    "model_video = VTCN(512, 44, [25]*8, 2, 0.2)\n",
    "output_video = model_video(x_video)\n",
    "print(\"video embedding vector size\", output_video.shape) # should be (10, 44)\n",
    "\n",
    "# Ecoding audio spectrogram with ImageTCN\n",
    "model_audio = ImageTCN(1, 44, [25]*8, 3, 0.2) # 1 input channel, 44 output classes, 8 layers, 25 channels per layer, kernel size 3, dropout 0.2\n",
    "output_audio = model_audio(x_audio)\n",
    "print(\"audio embedding vector size\", output_audio.shape) # should be (10, 44)\n",
    "\n",
    "# Ecoding egg spectrogram with ImageTCN\n",
    "model_egg = ImageTCN(32, 44, [25]*8, 3, 0.2)\n",
    "output_egg = model_egg(x_egg)\n",
    "print(\"egg embedding vector size\", output_egg.shape) # should be (10, 44)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
