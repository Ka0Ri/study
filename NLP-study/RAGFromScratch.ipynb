{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielWarfield1/MLWritingAndResearch/blob/main/RAGFromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cTk6k--hFjO"
      },
      "source": [
        "# RAG From Scratch\n",
        "This notebook is a low level conceptual exploration of RAG. We use a word vector encoder to embed words, calculate the mean vector of documents and prompts, and use manhattan distance as a distance metric.\n",
        "\n",
        "There are surely more efficient/better ways to get this done, which I'll explore in future demos. For now, this is the low level fundamentals.\n",
        "\n",
        "note:The terms \"embedding\" and \"encoding\" are painfully interchangable. Generally encoding is a verb, and an embedding is a noun, so you \"encode words into an embedding\", but it's also common to say you \"embed words into an embedding\". I have a tendency to flip between the two depending on the context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0Qaw64ejYdh"
      },
      "source": [
        "# Loading Word Space Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xj9HiosyhBPu",
        "outputId": "a3c330ba-cf4d-4589-fc7b-23f3f470bac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([ 0.85337  ,  0.011645 , -0.033377 , -0.31981  ,  0.26126  ,\n",
              "        0.16059  ,  0.010724 , -0.15542  ,  0.75044  ,  0.10688  ,\n",
              "        1.9249   , -0.45915  , -3.3887   , -1.2152   , -0.054263 ,\n",
              "       -0.20555  ,  0.54706  ,  0.4371   ,  0.25194  ,  0.0086557,\n",
              "       -0.56612  , -1.1762   ,  0.010479 , -0.55316  , -0.15816  ],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"Downloading a word encoder.\n",
        "I was going to use word2vect, but glove downloads way faster. For our purposes\n",
        "they're conceptually identical\n",
        "\"\"\"\n",
        "\n",
        "import gensim.downloader\n",
        "\n",
        "#doenloading encoder\n",
        "word_encoder = gensim.downloader.load('glove-twitter-25')\n",
        "\n",
        "#getting the embedding for a word\n",
        "word_encoder['apple']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i5XlcKAFmqR"
      },
      "source": [
        "# Embedding text\n",
        "embed either the document or the prompt via calculating the mean vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90Ri00AvA-t9",
        "outputId": "05e1e4d8-1a84-4a92-9ed9-2af9c6eb4a2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-6.3483393e-01,  1.3683620e-01,  2.0645106e-01, -2.1831200e-01,\n",
              "       -1.8181981e-01,  2.6023200e-01,  1.3276964e+00,  1.7272198e-01,\n",
              "       -2.7881199e-01, -4.2115799e-01, -4.7215199e-01, -5.3013992e-02,\n",
              "       -4.6326599e+00,  4.3883198e-01,  3.6487383e-01, -3.6672002e-01,\n",
              "       -2.6924044e-03, -3.0394283e-01, -5.5415201e-01, -9.1787003e-02,\n",
              "       -4.4997922e-01, -1.4819117e-01,  1.0654800e-01,  3.7024397e-01,\n",
              "       -4.6688594e-02], dtype=float32)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"defining a function for embedding an entire document to a single mean vector\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def embed_sequence(sequence):\n",
        "    vects = word_encoder[sequence.split(' ')]\n",
        "    return np.mean(vects, axis=0)\n",
        "\n",
        "embed_sequence('its a sunny day today')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-VuWZizI9BU"
      },
      "source": [
        "# Defining distance calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0CDcKq2HJRv",
        "outputId": "a9452ee7-cb98-41dd-af84-2b2397a39f25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "similar phrases:\n",
            "8.496297497302294\n",
            "different phrases:\n",
            "11.832107525318861\n"
          ]
        }
      ],
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def calc_distance(embedding1, embedding2):\n",
        "    return cdist(np.expand_dims(embedding1, axis=0), np.expand_dims(embedding2, axis=0), metric='cityblock')[0][0]\n",
        "\n",
        "print('similar phrases:')\n",
        "print(calc_distance(embed_sequence('sunny day today')\n",
        "                  , embed_sequence('rainy morning presently')))\n",
        "\n",
        "print('different phrases:')\n",
        "print(calc_distance(embed_sequence('sunny day today')\n",
        "                  , embed_sequence('perhaps reality is painful')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RPCor0TKRie"
      },
      "source": [
        "# Defining Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q-xcLiI4JtXY"
      },
      "outputs": [],
      "source": [
        "\"\"\"Defining documents\n",
        "for simplicities sake I only included words the embedder knows. You could just\n",
        "parse out all the words the embedder doesn't know, though. After all, the retreival\n",
        "is done on a mean of all embeddings, so a missing word or two is of little consequence\n",
        "\"\"\"\n",
        "documents = {\"menu\": \"ratatouille is a stew thats twelve dollars and fifty cents also gazpacho is a salad thats thirteen dollars and ninety eight cents also hummus is a dip thats eight dollars and seventy five cents also meat sauce is a pasta dish thats twelve dollars also penne marinera is a pasta dish thats eleven dollars also shrimp and linguini is a pasta dish thats fifteen dollars\",\n",
        "             \"events\": \"on thursday we have karaoke and on tuesdays we have trivia\",\n",
        "             \"allergins\": \"the only item on the menu common allergen is hummus which contain pine nuts\",\n",
        "             \"info\": \"the resteraunt was founded by two brothers in two thousand and three\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R58tdc2sSKNB"
      },
      "source": [
        "# Defining Retreival"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ibs6GhhwRric",
        "outputId": "1b4eec1b-dade-456e-e672-06b4ae6e024e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "finding relevent doc for \"what pasta dishes do you have\"\n",
            "('menu', 'ratatouille is a stew thats twelve dollars and fifty cents also gazpacho is a salad thats thirteen dollars and ninety eight cents also hummus is a dip thats eight dollars and seventy five cents also meat sauce is a pasta dish thats twelve dollars also penne marinera is a pasta dish thats eleven dollars also shrimp and linguini is a pasta dish thats fifteen dollars')\n",
            "----\n",
            "finding relevent doc for \"what events do you guys do\"\n",
            "('events', 'on thursday we have karaoke and on tuesdays we have trivia')\n"
          ]
        }
      ],
      "source": [
        "\"\"\"defining a function that retreives the most relevent document\n",
        "\"\"\"\n",
        "\n",
        "def retreive_relevent(prompt, documents=documents):\n",
        "    min_dist = 1000000000\n",
        "    r_docname = \"\"\n",
        "    r_doc = \"\"\n",
        "\n",
        "    for docname, doc in documents.items():\n",
        "        dist = calc_distance(embed_sequence(prompt)\n",
        "                           , embed_sequence(doc))\n",
        "\n",
        "        if dist < min_dist:\n",
        "            min_dist = dist\n",
        "            r_docname = docname\n",
        "            r_doc = doc\n",
        "\n",
        "    return r_docname, r_doc\n",
        "\n",
        "\n",
        "prompt = 'what pasta dishes do you have'\n",
        "print(f'finding relevent doc for \"{prompt}\"')\n",
        "print(retreive_relevent(prompt))\n",
        "print('----')\n",
        "prompt = 'what events do you guys do'\n",
        "print(f'finding relevent doc for \"{prompt}\"')\n",
        "print(retreive_relevent(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EalA9JmkZ4xg"
      },
      "source": [
        "# Defining Retreival and Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWXhRnTzZ4TB",
        "outputId": "70cad5a3-e5ed-4dd7-fa77-485158dc7f65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt for \"what events do you guys do\":\n",
            "\n",
            "Answer the customers prompt based on the folowing documents:\n",
            "==== document: events ====\n",
            "on thursday we have karaoke and on tuesdays we have trivia\n",
            "====\n",
            "\n",
            "prompt: what events do you guys do\n",
            "response:\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Defining retreival and augmentation\n",
        "creating a function that does retreival and augmentation,\n",
        "this can be passed straight to the model\n",
        "\"\"\"\n",
        "def retreive_and_agument(prompt, documents=documents):\n",
        "    docname, doc = retreive_relevent(prompt, documents)\n",
        "    return f\"Answer the customers prompt based on the folowing documents:\\n==== document: {docname} ====\\n{doc}\\n====\\n\\nprompt: {prompt}\\nresponse:\"\n",
        "\n",
        "prompt = 'what events do you guys do'\n",
        "print(f'prompt for \"{prompt}\":\\n')\n",
        "print(retreive_and_agument(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN3Czie_Vvl-"
      },
      "source": [
        "# Defining RAG and prompting OpenAI's LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaPOsnPKZGNX",
        "outputId": "23b2e1d4-82c1-4503-b73b-bcd9ed59eaf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Collecting openai\n",
            "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/1e/9f/385c25502f437686e4aa715969e5eaf5c2cb5e5ffa7c5cdd52f3c6ae967a/openai-0.28.1-py3-none-any.whl.metadata\n",
            "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /home/dspserver/anaconda3/envs/sd-ui/lib/python3.9/site-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /home/dspserver/anaconda3/envs/sd-ui/lib/python3.9/site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /home/dspserver/anaconda3/envs/sd-ui/lib/python3.9/site-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dspserver/anaconda3/envs/sd-ui/lib/python3.9/site-packages (from requests>=2.20->openai) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/dspserver/anaconda3/envs/sd-ui/lib/python3.9/site-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dspserver/anaconda3/envs/sd-ui/lib/python3.9/site-packages (from requests>=2.20->openai) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/dspserver/anaconda3/envs/sd-ui/lib/python3.9/site-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/dspserver/anaconda3/envs/sd-ui/lib/python3.9/site-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/dspserver/anaconda3/envs/sd-ui/lib/python3.9/site-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/dspserver/anaconda3/envs/sd-ui/lib/python3.9/site-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/dspserver/anaconda3/envs/sd-ui/lib/python3.9/site-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/dspserver/anaconda3/envs/sd-ui/lib/python3.9/site-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/dspserver/anaconda3/envs/sd-ui/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1)\n",
            "Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mDEPRECATION: torchsde 0.2.5 has a non-standard dependency specifier numpy>=1.19.*; python_version >= \"3.7\". pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of torchsde or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: openai\n",
            "Successfully installed openai-0.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ozb56SQ8YiJ0",
        "outputId": "b89c20b2-ba0f-4e13-af6e-5d3e54d06cb5"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/home/dspserver/share/Vu/RAGFromScratch.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.58/home/dspserver/share/Vu/RAGFromScratch.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#copying from google drive to local\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.58/home/dspserver/share/Vu/RAGFromScratch.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.58/home/dspserver/share/Vu/RAGFromScratch.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.58/home/dspserver/share/Vu/RAGFromScratch.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "#copying from google drive to local\n",
        "# from google.colab import drive\n",
        "# import os\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# with open (\"/content/drive/My Drive/Colab Notebooks/Credentials/OpenAI-danielDemoKey.txt\", \"r\") as myfile:\n",
        "#     OPENAI_API_TOKEN = myfile.read()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjQ3nLgyZErb",
        "outputId": "151f6809-8f66-4a52-c763-1a22de4546be"
      },
      "outputs": [
        {
          "ename": "RateLimitError",
          "evalue": "You exceeded your current quota, please check your plan and billing details.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/dspserver/share/Vu/RAGFromScratch.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.58/home/dspserver/share/Vu/RAGFromScratch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.58/home/dspserver/share/Vu/RAGFromScratch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     ra_prompt \u001b[39m=\u001b[39m retreive_and_agument(prompt)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.58/home/dspserver/share/Vu/RAGFromScratch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo-instruct\u001b[39;49m\u001b[39m\"\u001b[39;49m, prompt\u001b[39m=\u001b[39;49mra_prompt, max_tokens\u001b[39m=\u001b[39;49m\u001b[39m80\u001b[39;49m)\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.58/home/dspserver/share/Vu/RAGFromScratch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mprompt: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mprompt\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B168.131.153.58/home/dspserver/share/Vu/RAGFromScratch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mresponse: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/sd-ui/lib/python3.9/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[0;32m~/anaconda3/envs/sd-ui/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         deployment_id,\n\u001b[1;32m    142\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    157\u001b[0m         url,\n\u001b[1;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[0;32m~/anaconda3/envs/sd-ui/lib/python3.9/site-packages/openai/api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    280\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    290\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    291\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[0;32m--> 299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
            "File \u001b[0;32m~/anaconda3/envs/sd-ui/lib/python3.9/site-packages/openai/api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    703\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    704\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    707\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 710\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    711\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    712\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    713\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    714\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    715\u001b[0m         ),\n\u001b[1;32m    716\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    717\u001b[0m     )\n",
            "File \u001b[0;32m~/anaconda3/envs/sd-ui/lib/python3.9/site-packages/openai/api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    773\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    774\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    776\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    777\u001b[0m     )\n\u001b[1;32m    778\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
          ]
        }
      ],
      "source": [
        "\"\"\"Using RAG with OpenAI's gpt model\n",
        "\"\"\"\n",
        "\n",
        "import openai\n",
        "openai.api_key = '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'\n",
        "\n",
        "prompts = ['what pasta dishes do you have', 'what events do you guys do', 'oh cool what is karaoke']\n",
        "\n",
        "for prompt in prompts:\n",
        "\n",
        "    ra_prompt = retreive_and_agument(prompt)\n",
        "    response = openai.Completion.create(model=\"gpt-3.5-turbo-instruct\", prompt=ra_prompt, max_tokens=80).choices[0].text\n",
        "\n",
        "    print(f'prompt: \"{prompt}\"')\n",
        "    print(f'response: {response}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMkCp2DFyJCa1E9F3s7+Iz5",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
