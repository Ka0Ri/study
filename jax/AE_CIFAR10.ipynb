{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih_sQlw1qteS"
      },
      "source": [
        "# Tutorial 9 (JAX): Deep Autoencoders\n",
        "\n",
        "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=green)\n",
        "\n",
        "**Filled notebook:**\n",
        "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/JAX/tutorial9/AE_CIFAR10.ipynb)\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/JAX/tutorial9/AE_CIFAR10.ipynb)   \n",
        "**Pre-trained models:**\n",
        "[![View files on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/saved_models/tree/main/JAX/tutorial9)      \n",
        "**PyTorch version:**\n",
        "[![View on RTD](https://img.shields.io/static/v1.svg?logo=readthedocs&label=RTD&message=View%20On%20RTD&color=8CA1AF)](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial9/AE_CIFAR10.html)   \n",
        "**Author:** Phillip Lippe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuFeRfYdqteU"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "\n",
        "**Note:** This notebook is written in JAX+Flax. It is a 1-to-1 translation of the original notebook written in PyTorch+PyTorch Lightning with almost identical results. For an introduction to JAX, check out our [Tutorial 2 (JAX): Introduction to JAX+Flax](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.html). Further, throughout the notebook, we comment on major differences to the PyTorch version and provide explanations for the major parts of the JAX code.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05ruayJmqteV"
      },
      "source": [
        "---\n",
        "\n",
        "**Speed comparison**: We note the training times for all models in the PyTorch and the JAX implementation below (PyTorch v1.11, JAX v0.3.13). The models were trained on the same hardware (NVIDIA RTX3090, 24 core CPU) and we slightly adjusted the tutorials to use the exact same training settings (same data loading parameters, evaluation schedule, etc.). Overall, the JAX implementation is about *1.8x faster* than PyTorch!\n",
        "    \n",
        "| Models           |   PyTorch   |     JAX    |\n",
        "|------------------|:-----------:|:----------:|\n",
        "| AE - 64 latents  | 13min 10sec | 7min 10sec |\n",
        "| AE - 128 latents | 13min 11sec | 7min 10sec |\n",
        "| AE - 256 latents | 13min 11sec | 7min 11sec |\n",
        "| AE - 384 latents | 13min 12sec | 7min 14sec |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws_ES3EGqteV"
      },
      "source": [
        "In this tutorial, we will take a closer look at autoencoders (AE). Autoencoders are trained on encoding input data such as images into a smaller feature vector, and afterward, reconstruct it by a second neural network, called a decoder. The feature vector is called the \"bottleneck\" of the network as we aim to compress the input data into a smaller amount of features. This property is useful in many applications, in particular in compressing data or comparing images on a metric beyond pixel-level comparisons. Besides learning about the autoencoder framework, we will also see the \"deconvolution\" (or transposed convolution) operator in action for scaling up feature maps in height and width. Such deconvolution networks are necessary wherever we start from a small feature vector and need to output an image of full size (e.g. in VAE, GANs, or super-resolution applications).\n",
        "\n",
        "First of all, we import most of our standard libraries. We use [JAX](https://jax.readthedocs.io/en/latest/) as acceleration backend, [Flax](https://flax.readthedocs.io/en/latest/index.html) for implementing neural networks, and [Optax](https://optax.readthedocs.io/en/latest/index.html) to optimize the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcfYCGMuqteV",
        "outputId": "ebadcda0-1f2e-45ea-8881-5849f12d986c"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy import spatial\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "sns.set()\n",
        "\n",
        "## Progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "## To run JAX on TPU in Google Colab, uncomment the two lines below\n",
        "# import jax.tools.colab_tpu\n",
        "# jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "## JAX\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "# Seeding for random operations\n",
        "main_rng = random.PRNGKey(42)\n",
        "\n",
        "## Flax (NN in JAX)\n",
        "try:\n",
        "    import flax\n",
        "except ModuleNotFoundError: # Install flax if missing\n",
        "    !pip install --quiet flax\n",
        "    import flax\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state, checkpoints\n",
        "\n",
        "## Optax (Optimizers in JAX)\n",
        "try:\n",
        "    import optax\n",
        "except ModuleNotFoundError: # Install optax if missing\n",
        "    !pip install --quiet optax\n",
        "    import optax\n",
        "\n",
        "## PyTorch Data Loading\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "# Tensorboard extension (for visualization purposes later)\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"../../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"/home/dspserver/share/Vu/study/saved_models/tutorial9_jax\"\n",
        "\n",
        "print(\"Device:\", jax.devices()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x8rQxEeqteW"
      },
      "source": [
        "We have 4 pretrained models that we have to download. Remember the adjust the variables `DATASET_PATH` and `CHECKPOINT_PATH` if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJvk7wbJqteW"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "# Github URL where saved models are stored for this tutorial\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/JAX/tutorial9/\"\n",
        "# Files to download\n",
        "pretrained_files = [\"cifar10_64.ckpt\", \"cifar10_128.ckpt\", \"cifar10_256.ckpt\", \"cifar10_384.ckpt\"]\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for file_name in pretrained_files:\n",
        "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please contact the author with the full output including the following error:\\n\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLDsk0TqqteW"
      },
      "source": [
        "In this tutorial, we work with the CIFAR10 dataset. In CIFAR10, each image has 3 color channels and is 32x32 pixels large. As autoencoders do not have the constrain of modeling images probabilistic, we can work on more complex image data (i.e. 3 color channels instead of black-and-white) much easier than for VAEs.\n",
        "In case you have downloaded CIFAR10 already in a different directory, make sure to set DATASET_PATH accordingly to prevent another download.\n",
        "\n",
        "In contrast to previous tutorials on CIFAR10 like [Tutorial 5](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial5/Inception_ResNet_DenseNet.html) (CNN classification), we do not normalize the data explicitly with a mean of 0 and std of 1, but roughly estimate it scaling the data between -1 and 1. This is because limiting the range will make our task of predicting/reconstructing images easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbLS6PewqteX",
        "outputId": "0d815f03-bb46-410e-ec53-2baf3c9da32a"
      },
      "outputs": [],
      "source": [
        "# Transformations applied on each image => bring them into a numpy array\n",
        "def image_to_numpy(img):\n",
        "    img = np.array(img, dtype=np.float32)\n",
        "    if img.max() > 1:\n",
        "        img = img / 255. * 2. - 1.\n",
        "    return img\n",
        "\n",
        "# For visualization, we might want to map JAX or numpy tensors back to PyTorch\n",
        "def jax_to_torch(imgs):\n",
        "    imgs = jax.device_get(imgs)\n",
        "    imgs = torch.from_numpy(imgs.astype(np.float32))\n",
        "    imgs = imgs.permute(0, 3, 1, 2)\n",
        "    return imgs\n",
        "\n",
        "# We need to stack the batch elements\n",
        "def numpy_collate(batch):\n",
        "    if isinstance(batch[0], np.ndarray):\n",
        "        return np.stack(batch)\n",
        "    elif isinstance(batch[0], (tuple,list)):\n",
        "        transposed = zip(*batch)\n",
        "        return [numpy_collate(samples) for samples in transposed]\n",
        "    else:\n",
        "        return np.array(batch)\n",
        "\n",
        "# Loading the training dataset. We need to split it into a training and validation part\n",
        "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=image_to_numpy, download=True)\n",
        "train_set, val_set = data.random_split(train_dataset, [45000, 5000], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "# Loading the test set\n",
        "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=image_to_numpy, download=True)\n",
        "\n",
        "# We define a set of data loaders that we can use for various purposes later.\n",
        "train_loader = data.DataLoader(train_set, batch_size=256, shuffle=True, drop_last=True, pin_memory=True, num_workers=4, collate_fn=numpy_collate, persistent_workers=True)\n",
        "val_loader = data.DataLoader(val_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4, collate_fn=numpy_collate)\n",
        "test_loader = data.DataLoader(test_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4, collate_fn=numpy_collate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXVG0Z14qteX"
      },
      "source": [
        "## Building the autoencoder\n",
        "\n",
        "In general, an autoencoder consists of an **encoder** that maps the input $x$ to a lower-dimensional feature vector $z$, and a **decoder** that reconstructs the input $\\hat{x}$ from $z$. We train the model by comparing $x$ to $\\hat{x}$ and optimizing the parameters to increase the similarity between $x$ and $\\hat{x}$. See below for a small illustration of the autoencoder framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE5Z5J7_qteX"
      },
      "source": [
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial9/autoencoder_visualization.svg?raw=1\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"650px\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eynZjz9kqteX"
      },
      "source": [
        "We first start by implementing the encoder. The encoder effectively consists of a deep convolutional network, where we scale down the image layer-by-layer using strided convolutions. After downscaling the image three times, we flatten the features and apply linear layers. The latent representation $z$ is therefore a vector of size *d* which can be flexibly selected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6VyYYJpqteX"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    c_hid : int\n",
        "    latent_dim : int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Conv(features=self.c_hid, kernel_size=(3, 3), strides=2)(x)  # 32x32 => 16x16\n",
        "        x = nn.gelu(x)\n",
        "        x = nn.Conv(features=self.c_hid, kernel_size=(3, 3))(x)\n",
        "        x = nn.gelu(x)\n",
        "        x = nn.Conv(features=2*self.c_hid, kernel_size=(3, 3), strides=2)(x)  # 16x16 => 8x8\n",
        "        x = nn.gelu(x)\n",
        "        x = nn.Conv(features=2*self.c_hid, kernel_size=(3, 3))(x)\n",
        "        x = nn.gelu(x)\n",
        "        x = nn.Conv(features=2*self.c_hid, kernel_size=(3, 3), strides=2)(x)  # 8x8 => 4x4\n",
        "        x = nn.gelu(x)\n",
        "        x = x.reshape(x.shape[0], -1)  # Image grid to single feature vector\n",
        "        x = nn.Dense(features=self.latent_dim)(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G1epvP8qteX"
      },
      "outputs": [],
      "source": [
        "## Test encoder implementation\n",
        "# Random key for initialization\n",
        "rng = random.PRNGKey(0)\n",
        "# Example images as input\n",
        "imgs = next(iter(train_loader))[0]\n",
        "# Create encoder\n",
        "encoder = Encoder(c_hid=32, latent_dim=128)\n",
        "# Initialize parameters of encoder with random key and images\n",
        "params = encoder.init(rng, imgs)['params']\n",
        "# Apply encoder with parameters on the images\n",
        "out = encoder.apply({'params': params}, imgs)\n",
        "out.shape\n",
        "\n",
        "del out, encoder, params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDNocolHqteX"
      },
      "source": [
        "Note that we do not apply Batch Normalization here. This is because we want the encoding of each image to be independent of all the other images. Otherwise, we might introduce correlations into the encoding or decoding that we do not want to have. In some implementations, you still can see Batch Normalization being used, because it can also serve as a form of regularization. Nevertheless, the better practice is to go with other normalization techniques if necessary like Instance Normalization or Layer Normalization. Given the small size of the model, we can neglect normalization for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5_PVFvaqteX"
      },
      "source": [
        "The decoder is a mirrored, flipped version of the encoder. The only difference is that we replace strided convolutions by transposed convolutions (i.e. deconvolutions) to upscale the features. Transposed convolutions can be imagined as adding the stride to the input instead of the output, and can thus upscale the input. For an illustration of a `nn.ConvTranspose` layer with kernel size 3, stride 2, and padding 1, see below (figure credit - [Vincent Dumoulin and Francesco Visin](https://arxiv.org/abs/1603.07285)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial9/deconvolution.gif?raw=1\" width=\"250px\"></center>\n",
        "\n",
        "You see that for an input of size $3\\times3$, we obtain an output of $5\\times5$. However, to truly have a reverse operation of the convolution, we need to ensure that the layer scales the input shape by a factor of 2 (e.g. $4\\times4\\to8\\times8$). Flax already has this as a default setting for the padding, so we do not need to adjust anything here.\n",
        "\n",
        "Overall, the decoder can be implemented as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzgJnaBcqteX"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    c_out : int\n",
        "    c_hid : int\n",
        "    latent_dim : int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Dense(features=2*16*self.c_hid)(x)\n",
        "        x = nn.gelu(x)\n",
        "        x = x.reshape(x.shape[0], 4, 4, -1)\n",
        "        x = nn.ConvTranspose(features=2*self.c_hid, kernel_size=(3, 3), strides=(2, 2))(x)\n",
        "        x = nn.gelu(x)\n",
        "        x = nn.Conv(features=2*self.c_hid, kernel_size=(3, 3))(x)\n",
        "        x = nn.gelu(x)\n",
        "        x = nn.ConvTranspose(features=self.c_hid, kernel_size=(3, 3), strides=(2, 2))(x)\n",
        "        x = nn.gelu(x)\n",
        "        x = nn.Conv(features=self.c_hid, kernel_size=(3, 3))(x)\n",
        "        x = nn.gelu(x)\n",
        "        x = nn.ConvTranspose(features=self.c_out, kernel_size=(3, 3), strides=(2, 2))(x)\n",
        "        x = nn.tanh(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3pv0czbqteY"
      },
      "outputs": [],
      "source": [
        "## Test decoder implementation\n",
        "# Random key for initialization\n",
        "rng = random.PRNGKey(0)\n",
        "# Example latents as input\n",
        "rng, lat_rng = random.split(rng)\n",
        "latents = random.normal(lat_rng, (16, 128))\n",
        "# Create decoder\n",
        "decoder = Decoder(c_hid=32, latent_dim=128, c_out=3)\n",
        "# Initialize parameters of decoder with random key and latents\n",
        "rng, init_rng = random.split(rng)\n",
        "params = decoder.init(init_rng, latents)['params']\n",
        "# Apply decoder with parameters on the images\n",
        "out = decoder.apply({'params': params}, latents)\n",
        "out.shape\n",
        "\n",
        "del out, decoder, params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cWJWoLBqteY"
      },
      "source": [
        "The encoder and decoder networks we chose here are relatively simple. Usually, more complex networks are applied, especially when using a ResNet-based architecture. For example, see [VQ-VAE](https://arxiv.org/abs/1711.00937) and [NVAE](https://arxiv.org/abs/2007.03898) (although the papers discuss architectures for VAEs, they can equally be applied to standard autoencoders).\n",
        "\n",
        "In a final step, we add the encoder and decoder together into the autoencoder architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GEYtXdBqteY"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    c_hid: int\n",
        "    latent_dim : int\n",
        "\n",
        "    def setup(self):\n",
        "        # Alternative to @nn.compact -> explicitly define modules\n",
        "        # Better for later when we want to access the encoder and decoder explicitly\n",
        "        self.encoder = Encoder(c_hid=self.c_hid, latent_dim=self.latent_dim)\n",
        "        self.decoder = Decoder(c_hid=self.c_hid, latent_dim=self.latent_dim, c_out=3)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "        return x_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6G_DZvCqteY"
      },
      "outputs": [],
      "source": [
        "## Test Autoencoder implementation\n",
        "# Random key for initialization\n",
        "rng = random.PRNGKey(0)\n",
        "# Example images as input\n",
        "imgs = next(iter(train_loader))[0]\n",
        "# Create encoder\n",
        "autoencoder = Autoencoder(c_hid=32, latent_dim=128)\n",
        "# Initialize parameters of encoder with random key and images\n",
        "params = autoencoder.init(rng, imgs)['params']\n",
        "# Apply encoder with parameters on the images\n",
        "out = autoencoder.apply({'params': params}, imgs)\n",
        "out.shape\n",
        "\n",
        "del out, autoencoder, params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om72QUdIqteY"
      },
      "source": [
        "For the loss function, we use the mean squared error (MSE), which we implement below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBcxYYYhqteY"
      },
      "outputs": [],
      "source": [
        "def mse_recon_loss(model, params, batch):\n",
        "    imgs, _ = batch\n",
        "    recon_imgs = model.apply({'params': params}, imgs)\n",
        "    loss = ((recon_imgs - imgs) ** 2).mean(axis=0).sum()  # Mean over batch, sum over pixels\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nji_gN73qteY"
      },
      "source": [
        "The mean squared error pushes the network to pay special attention to those pixel values its estimate is far away. Predicting 127 instead of 128 is not important when reconstructing, but confusing 0 with 128 is much worse. Note that in contrast to VAEs, we do not predict the probability per pixel value, but instead use a distance measure. This saves a lot of parameters and simplifies training. To get a better intuition per pixel, we report the summed squared error averaged over the batch dimension (any other mean/sum leads to the same result/parameters).\n",
        "\n",
        "However, MSE has also some considerable disadvantages. Usually, MSE leads to blurry images where small noise/high-frequent patterns are removed as those cause a very low error. To ensure realistic images to be reconstructed, one could combine Generative Adversarial Networks (lecture 10) with autoencoders as done in several works (e.g. see [here](https://arxiv.org/abs/1704.02304), [here](https://arxiv.org/abs/1511.05644) or these [slides](http://elarosca.net/slides/iccv_autoencoder_gans.pdf)). Additionally, comparing two images using MSE does not necessarily reflect their visual similarity. For instance, suppose the autoencoder reconstructs an image shifted by one pixel to the right and bottom. Although the images are almost identical, we can get a higher loss than predicting a constant pixel value for half of the image (see code below). An example solution for this issue includes using a separate, pre-trained CNN, and use a distance of visual features in lower layers as a distance measure instead of the original pixel-level comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtbFjyiFqteY",
        "outputId": "9e022664-37f6-4fe7-85b9-5013d704b7ff"
      },
      "outputs": [],
      "source": [
        "def compare_imgs(img1, img2, title_prefix=\"\"):\n",
        "    # Calculate MSE loss between both images\n",
        "    loss = ((img1 - img2) ** 2).sum()\n",
        "    # Plot images for visual comparison\n",
        "    imgs = jax_to_torch(np.stack([img1, img2], axis=0))\n",
        "    grid = torchvision.utils.make_grid(imgs, nrow=2, normalize=True, value_range=(-1,1))\n",
        "    grid = grid.permute(1, 2, 0)\n",
        "    plt.figure(figsize=(4,2))\n",
        "    plt.title(f\"{title_prefix} Loss: {loss.item():4.2f}\")\n",
        "    plt.imshow(grid)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "for i in range(2):\n",
        "    # Load example image\n",
        "    img, _ = train_dataset[i]\n",
        "    img_mean = img.mean(axis=(0,1), keepdims=True)\n",
        "\n",
        "    # Shift image by one pixel\n",
        "    SHIFT = 1\n",
        "    img_shifted = np.roll(img, shift=SHIFT, axis=0)\n",
        "    img_shifted = np.roll(img_shifted, shift=SHIFT, axis=1)\n",
        "    img_shifted[:1,:,:] = img_mean\n",
        "    img_shifted[:,:1,:] = img_mean\n",
        "    compare_imgs(img, img_shifted, \"Shifted -\")\n",
        "\n",
        "    # Set half of the image to zero\n",
        "    img_masked = np.copy(img)\n",
        "    img_masked[:img_masked.shape[1]//2,:,:] = img_mean\n",
        "    compare_imgs(img, img_masked, \"Masked -\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzsXjVpBqteY"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "During the training, we want to keep track of the learning progress by seeing reconstructions made by our model. For this, we implement a callback object which will add reconstructions every $N$ epochs to our tensorboard. To align it with the PyTorch tutorial version, we implement it similar to how we would do it in PyTorch Lightning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJpreAvxqteY"
      },
      "outputs": [],
      "source": [
        "class GenerateCallback:\n",
        "\n",
        "    def __init__(self, input_imgs, every_n_epochs=1):\n",
        "        super().__init__()\n",
        "        self.input_imgs = input_imgs  # Images to reconstruct during training\n",
        "        self.every_n_epochs = every_n_epochs  # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
        "\n",
        "    def log_generations(self, model, state, logger, epoch):\n",
        "        if epoch % self.every_n_epochs == 0:\n",
        "            reconst_imgs = model.apply({'params': state.params}, self.input_imgs)\n",
        "            reconst_imgs = jax.device_get(reconst_imgs)\n",
        "\n",
        "            # Plot and add to tensorboard\n",
        "            imgs = np.stack([self.input_imgs, reconst_imgs], axis=1).reshape(-1, *self.input_imgs.shape[1:])\n",
        "            imgs = jax_to_torch(imgs)\n",
        "            grid = torchvision.utils.make_grid(imgs, nrow=2, normalize=True, value_range=(-1,1))\n",
        "            logger.add_image(\"Reconstructions\", grid, global_step=epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4-jIIycqteY"
      },
      "source": [
        "Further, to train multiple models with different hyperparameters, we summarize all training functionalities in a trainer object below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x53pgjl2qteY"
      },
      "outputs": [],
      "source": [
        "class TrainerModule:\n",
        "\n",
        "    def __init__(self, c_hid, latent_dim, lr=1e-3, seed=42):\n",
        "        super().__init__()\n",
        "        self.c_hid = c_hid\n",
        "        self.latent_dim = latent_dim\n",
        "        self.lr = lr\n",
        "        self.seed = seed\n",
        "        # Create empty model. Note: no parameters yet\n",
        "        self.model = Autoencoder(c_hid=self.c_hid, latent_dim=self.latent_dim)\n",
        "        # Prepare logging\n",
        "        self.exmp_imgs = next(iter(val_loader))[0][:8]\n",
        "        self.log_dir = os.path.join(CHECKPOINT_PATH, f'cifar10_{self.latent_dim}')\n",
        "        self.generate_callback = GenerateCallback(self.exmp_imgs, every_n_epochs=50)\n",
        "        self.logger = SummaryWriter(log_dir=self.log_dir)\n",
        "        # Create jitted training and eval functions\n",
        "        self.create_functions()\n",
        "        # Initialize model\n",
        "        self.init_model()\n",
        "\n",
        "    def create_functions(self):\n",
        "        # Training function\n",
        "        def train_step(state, batch):\n",
        "            loss_fn = lambda params: mse_recon_loss(self.model, params, batch)\n",
        "            loss, grads = jax.value_and_grad(loss_fn)(state.params)  # Get loss and gradients for loss\n",
        "            state = state.apply_gradients(grads=grads)  # Optimizer update step\n",
        "            return state, loss\n",
        "        self.train_step = jax.jit(train_step)\n",
        "        # Eval function\n",
        "        def eval_step(state, batch):\n",
        "            return mse_recon_loss(self.model, state.params, batch)\n",
        "        self.eval_step = jax.jit(eval_step)\n",
        "\n",
        "    def init_model(self):\n",
        "        # Initialize model\n",
        "        rng = jax.random.PRNGKey(self.seed)\n",
        "        rng, init_rng = jax.random.split(rng)\n",
        "        params = self.model.init(init_rng, self.exmp_imgs)['params']\n",
        "        # Initialize learning rate schedule and optimizer\n",
        "        lr_schedule = optax.warmup_cosine_decay_schedule(\n",
        "            init_value=0.0,\n",
        "            peak_value=1e-3,\n",
        "            warmup_steps=100,\n",
        "            decay_steps=500*len(train_loader),\n",
        "            end_value=1e-5\n",
        "        )\n",
        "        optimizer = optax.chain(\n",
        "            optax.clip(1.0),  # Clip gradients at 1\n",
        "            optax.adam(lr_schedule)\n",
        "        )\n",
        "        # Initialize training state\n",
        "        self.state = train_state.TrainState.create(apply_fn=self.model.apply, params=params, tx=optimizer)\n",
        "\n",
        "    def train_model(self, num_epochs=500):\n",
        "        # Train model for defined number of epochs\n",
        "        best_eval = 1e6\n",
        "        for epoch_idx in tqdm(range(1, num_epochs+1)):\n",
        "            self.train_epoch(epoch=epoch_idx)\n",
        "            if epoch_idx % 10 == 0:\n",
        "                eval_loss = self.eval_model(val_loader)\n",
        "                self.logger.add_scalar('val/loss', eval_loss, global_step=epoch_idx)\n",
        "                if eval_loss < best_eval:\n",
        "                    best_eval = eval_loss\n",
        "                    self.save_model(step=epoch_idx)\n",
        "                self.generate_callback.log_generations(self.model, self.state, logger=self.logger, epoch=epoch_idx)\n",
        "                self.logger.flush()\n",
        "\n",
        "    def train_epoch(self, epoch):\n",
        "        # Train model for one epoch, and log avg loss\n",
        "        losses = []\n",
        "        for batch in train_loader:\n",
        "            self.state, loss = self.train_step(self.state, batch)\n",
        "            losses.append(loss)\n",
        "        losses_np = np.stack(jax.device_get(losses))\n",
        "        avg_loss = losses_np.mean()\n",
        "        self.logger.add_scalar('train/loss', avg_loss, global_step=epoch)\n",
        "\n",
        "    def eval_model(self, data_loader):\n",
        "        # Test model on all images of a data loader and return avg loss\n",
        "        losses = []\n",
        "        batch_sizes = []\n",
        "        for batch in data_loader:\n",
        "            loss = self.eval_step(self.state, batch)\n",
        "            losses.append(loss)\n",
        "            batch_sizes.append(batch[0].shape[0])\n",
        "        losses_np = np.stack(jax.device_get(losses))\n",
        "        batch_sizes_np = np.stack(batch_sizes)\n",
        "        avg_loss = (losses_np * batch_sizes_np).sum() / batch_sizes_np.sum()\n",
        "        return avg_loss\n",
        "\n",
        "    def save_model(self, step=0):\n",
        "        # Save current model at certain training iteration\n",
        "        checkpoints.save_checkpoint(ckpt_dir=self.log_dir, target=self.state.params, prefix=f'cifar10_{self.latent_dim}_', step=step)\n",
        "\n",
        "    def load_model(self, pretrained=False):\n",
        "        # Load model. We use different checkpoint for pretrained models\n",
        "        if not pretrained:\n",
        "            params = checkpoints.restore_checkpoint(ckpt_dir=self.log_dir, target=self.state.params, prefix=f'cifar10_{self.latent_dim}_')\n",
        "        else:\n",
        "            params = checkpoints.restore_checkpoint(ckpt_dir=os.path.join(CHECKPOINT_PATH, f'cifar10_{self.latent_dim}.ckpt'), target=self.state.params)\n",
        "        self.state = train_state.TrainState.create(apply_fn=self.model.apply, params=params, tx=self.state.tx)\n",
        "\n",
        "    def checkpoint_exists(self):\n",
        "        # Check whether a pretrained model exist for this autoencoder\n",
        "        return os.path.isfile(os.path.join(CHECKPOINT_PATH, f'cifar10_{self.latent_dim}.ckpt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB35zNLIqteY"
      },
      "source": [
        "We will now write a training function that allows us to train the autoencoder with different latent dimensionality and returns the test score. We provide pre-trained models and recommend you using those, especially when you work on a computer without GPU. Of course, feel free to train your own models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRWLSmu3qteY"
      },
      "outputs": [],
      "source": [
        "def train_cifar(latent_dim):\n",
        "    # Create a trainer module with specified hyperparameters\n",
        "    trainer = TrainerModule(c_hid=32, latent_dim=latent_dim)\n",
        "    if True: # not trainer.checkpoint_exists():  Skip training if pretrained model exists\n",
        "        trainer.train_model(num_epochs=500)\n",
        "        trainer.load_model()\n",
        "    else:\n",
        "        trainer.load_model(pretrained=True)\n",
        "    test_loss = trainer.eval_model(test_loader)\n",
        "    # Bind parameters to model for easier inference\n",
        "    trainer.model_bd = trainer.model.bind({'params': trainer.state.params})\n",
        "    return trainer, test_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxnBRZP2qteY"
      },
      "source": [
        "### Comparing latent dimensionality\n",
        "\n",
        "When training an autoencoder, we need to choose a dimensionality for the latent representation $z$. The higher the latent dimensionality, the better we expect the reconstruction to be. However, the idea of autoencoders is to *compress* data. Hence, we are also interested in keeping the dimensionality low. To find the best tradeoff, we can train multiple models with different latent dimensionalities. The original input has $32\\times 32\\times 3 = 3072$ pixels. Keeping this in mind, a reasonable choice for the latent dimensionality might be between 64 and 384:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1Sda2exqteY"
      },
      "outputs": [],
      "source": [
        "model_dict = {}\n",
        "for latent_dim in [64, 128, 256, 384]:\n",
        "    trainer_ld, test_loss_ld = train_cifar(latent_dim)\n",
        "    model_dict[latent_dim] = {\"trainer\": trainer_ld, \"result\": test_loss_ld}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tsf7KrxQqteZ"
      },
      "source": [
        "After training the models, we can plot the reconstruction loss over the latent dimensionality to get an intuition how these two properties are correlated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO00DiyIqteZ",
        "outputId": "bc3b40ba-3844-4691-e9fd-52c0e964aea3"
      },
      "outputs": [],
      "source": [
        "latent_dims = sorted([k for k in model_dict])\n",
        "val_scores = [model_dict[k][\"result\"] for k in latent_dims]\n",
        "\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "plt.plot(latent_dims, val_scores, '--', color=\"#000\", marker=\"*\", markeredgecolor=\"#000\", markerfacecolor=\"y\", markersize=16)\n",
        "plt.xscale(\"log\")\n",
        "plt.xticks(latent_dims, labels=latent_dims)\n",
        "plt.title(\"Reconstruction error over latent dimensionality\", fontsize=14)\n",
        "plt.xlabel(\"Latent dimensionality\")\n",
        "plt.ylabel(\"Reconstruction error\")\n",
        "plt.minorticks_off()\n",
        "plt.ylim(0,100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0iuVkSeqteZ"
      },
      "source": [
        "As we initially expected, the reconstruction loss goes down with increasing latent dimensionality. For our model and setup, the two properties seem to be exponentially (or double exponentially) correlated. To understand what these differences in reconstruction error mean, we can visualize example reconstructions of the four models. For simplicity, we visualize four training images of CIFAR10 we have seen already before. For larger models that may overfit, it is recommended to use images from the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgVwXyMuqteZ"
      },
      "outputs": [],
      "source": [
        "def visualize_reconstructions(trainer, input_imgs):\n",
        "    # Reconstruct images\n",
        "    reconst_imgs = trainer.model_bd(input_imgs)\n",
        "    imgs = np.stack([input_imgs, reconst_imgs], axis=1).reshape(-1, *reconst_imgs.shape[1:])\n",
        "\n",
        "    # Plotting\n",
        "    imgs = jax_to_torch(imgs)\n",
        "    grid = torchvision.utils.make_grid(imgs, nrow=4, normalize=True, value_range=(-1,1))\n",
        "    grid = grid.permute(1, 2, 0)\n",
        "    plt.figure(figsize=(7,4.5))\n",
        "    plt.title(f\"Reconstructed from {trainer.latent_dim} latents\")\n",
        "    plt.imshow(grid)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4HobivmqteZ",
        "outputId": "f936c3de-2679-4ba3-edd7-3a4d3276d84d"
      },
      "outputs": [],
      "source": [
        "input_imgs = np.stack([image_to_numpy(train_dataset[i][0]) for i in range(4)], axis=0)\n",
        "for latent_dim in model_dict:\n",
        "    visualize_reconstructions(model_dict[latent_dim][\"trainer\"], input_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPpYHKI3qteZ"
      },
      "source": [
        "Clearly, the smallest latent dimensionality can only save information about the rough shape and color of the object, but the reconstructed image is extremely blurry and it is hard to recognize the original object in the reconstruction. With 128 features, we can recognize some shapes again although the picture remains blurry. The models with the highest two dimensionalities reconstruct the images quite well. The difference between 256 and 384 is marginal at first sight but can be noticed when comparing, for instance, the backgrounds of the first image (the 384 features model more of the pattern than 256)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4jYUcIIqteb"
      },
      "source": [
        "### Out-of-distribution images\n",
        "\n",
        "Before continuing with the applications of autoencoder, we can actually explore some limitations of our autoencoder. For example, what happens if we try to reconstruct an image that is clearly out of the distribution of our dataset? We expect the decoder to have learned some common patterns in the dataset, and thus might in particular fail to reconstruct images that do not follow these patterns.\n",
        "\n",
        "The first experiment we can try is to reconstruct noise. We, therefore, create two images whose pixels are randomly sampled from a uniform distribution over pixel values, and visualize the reconstruction of the model (feel free to test different latent dimensionalities):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVxombgDqteb",
        "outputId": "92225340-31ba-4e6e-87e6-9a2e558b9b06"
      },
      "outputs": [],
      "source": [
        "rng = jax.random.PRNGKey(123)\n",
        "rgn, noise_rgn = jax.random.split(rng)\n",
        "rand_imgs = jax.random.uniform(rng, (2, 32, 32, 3)) * 2 - 1\n",
        "visualize_reconstructions(model_dict[256][\"trainer\"], rand_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4tm9UUiqtec"
      },
      "source": [
        "The reconstruction of the noise is quite poor, and seems to introduce some rough patterns. As the input does not follow the patterns of the CIFAR dataset, the model has issues reconstructing it accurately.\n",
        "\n",
        "We can also check how well the model can reconstruct other manually-coded patterns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FacLSpC0qtec",
        "outputId": "bba7ee4c-2852-4180-c74a-1e9900178a31"
      },
      "outputs": [],
      "source": [
        "# JAX arrays are natively immutable. This is why we first create the images in numpy, and push them to JAX afterwards\n",
        "plain_imgs = np.zeros((4, 32, 32, 3))\n",
        "\n",
        "# Single color channel\n",
        "plain_imgs[1,:,:,0] = 1\n",
        "# Checkboard pattern\n",
        "plain_imgs[2,:16,:16] = 1\n",
        "plain_imgs[2,16:,16:] = -1\n",
        "# Color progression\n",
        "xx, yy = np.meshgrid(np.linspace(-1,1,32), np.linspace(-1,1,32), indexing='ij')\n",
        "plain_imgs[3,:,:,0] = xx\n",
        "plain_imgs[3,:,:,1] = yy\n",
        "\n",
        "visualize_reconstructions(model_dict[256][\"trainer\"], plain_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM8iwCC7qtec"
      },
      "source": [
        "The plain, constant images are reconstructed relatively good although the single color channel contains some noticeable noise. The hard borders of the checkboard pattern are not as sharp as intended, as well as the color progression, both because such patterns never occur in the real-world pictures of CIFAR.\n",
        "\n",
        "In general, autoencoders tend to fail reconstructing high-frequent noise (i.e. sudden, big changes across few pixels) due to the choice of MSE as loss function (see our previous discussion about loss functions in autoencoders). Small misalignments in the decoder can lead to huge losses so that the model settles for the expected value/mean in these regions. For low-frequent noise, a misalignment of a few pixels does not result in a big difference to the original image. However, the larger the latent dimensionality becomes, the more of this high-frequent noise can be accurately reconstructed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rgIxrfCqtec"
      },
      "source": [
        "### Generating new images\n",
        "\n",
        "Variational autoencoders are a generative version of the autoencoders because we regularize the latent space to follow a Gaussian distribution. However, in vanilla autoencoders, we do not have any restrictions on the latent vector. So what happens if we would actually input a randomly sampled latent vector into the decoder? Let's find it out below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UCjC8MTqtec",
        "outputId": "76ad458b-e4e6-4f58-e2be-93e97ea48e3b"
      },
      "outputs": [],
      "source": [
        "trainer = model_dict[256][\"trainer\"]\n",
        "rgn, latent_noise = jax.random.split(rng)\n",
        "latent_vectors = jax.random.normal(latent_noise, (8, trainer.latent_dim))\n",
        "# Decode images -> Run model.decode method of the trainer's model with given parameters\n",
        "imgs = trainer.model_bd.decoder(latent_vectors) # nn.apply(lambda model: model.decode(latent_vectors), trainer.model)({'params': trainer.state.params})\n",
        "\n",
        "imgs = jax_to_torch(imgs)\n",
        "grid = torchvision.utils.make_grid(imgs, nrow=4, normalize=True, value_range=(-1,1), pad_value=0.5)\n",
        "grid = grid.permute(1, 2, 0)\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.imshow(grid)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drEMPHsfqtec"
      },
      "source": [
        "As we can see, the generated images more look like art than realistic images. As the autoencoder was allowed to structure the latent space in whichever way it suits the reconstruction best, there is no incentive to map every possible latent vector to realistic images. Furthermore, the distribution in latent space is unknown to us and doesn't necessarily follow a multivariate normal distribution. Thus, we can conclude that vanilla autoencoders are indeed not generative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4_PfW0Dqtec"
      },
      "source": [
        "## Finding visually similar images\n",
        "\n",
        "One application of autoencoders is to build an image-based search engine to retrieve visually similar images. This can be done by representing all images as their latent dimensionality, and find the closest $K$ images in this domain. The first step to such a search engine is to encode all images into $z$. In the following, we will use the training set as a search corpus, and the test set as queries to the system.\n",
        "\n",
        "<div class=\"alert alert-warning\">\n",
        "\n",
        "**Warning:** the following cells can be computationally heavy for a weak CPU-only system. If you do not have a strong computer and are not on Google Colab, you might want to skip the execution of the following cells and rely on the results shown in the filled notebook.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLalRLORqtec"
      },
      "outputs": [],
      "source": [
        "# We use the following model throughout this section.\n",
        "# If you want to try a different latent dimensionality, change it here!\n",
        "trainer = model_dict[128][\"trainer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            ""
          ]
        },
        "id": "Ev0ocyB3qtec",
        "outputId": "91ed77f2-7c13-487c-adb8-409b97a45133"
      },
      "outputs": [],
      "source": [
        "def embed_imgs(trainer, data_loader):\n",
        "    # Encode all images in the data_laoder using model, and return both images and encodings\n",
        "    img_list, embed_list = [], []\n",
        "\n",
        "    @jax.jit\n",
        "    def encode(imgs):\n",
        "        return trainer.model.bind({'params': trainer.state.params}).encoder(imgs)\n",
        "\n",
        "    for imgs, _ in tqdm(data_loader, desc=\"Encoding images\", leave=False):\n",
        "        z = encode(imgs)\n",
        "        z = jax.device_get(z)\n",
        "        imgs = jax.device_get(imgs)\n",
        "        img_list.append(imgs)\n",
        "        embed_list.append(z)\n",
        "    return (np.concatenate(img_list, axis=0), np.concatenate(embed_list, axis=0))\n",
        "\n",
        "train_img_embeds = embed_imgs(trainer, train_loader)\n",
        "test_img_embeds = embed_imgs(trainer, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF76KieNqtec"
      },
      "source": [
        "After encoding all images, we just need to write a function that finds the closest $K$ images and returns (or plots) those:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNmyMeHBqtec"
      },
      "outputs": [],
      "source": [
        "def find_similar_images(query_img, query_z, key_embeds, K=8):\n",
        "    # Find closest K images. We use the euclidean distance here but other like cosine distance can also be used.\n",
        "    dist = np.linalg.norm(query_z[None,:] - key_embeds[1], axis=-1)\n",
        "    indices = np.argsort(dist)\n",
        "    dist = dist[indices]\n",
        "    # Plot K closest images\n",
        "    imgs_to_display = np.concatenate([query_img[None], key_embeds[0][indices[:K]]], axis=0)\n",
        "    imgs_to_display = torch.from_numpy(imgs_to_display)\n",
        "    imgs_to_display = imgs_to_display.permute(0, 3, 1, 2)\n",
        "    grid = torchvision.utils.make_grid(imgs_to_display, nrow=K+1, normalize=True, value_range=(-1,1))\n",
        "    grid = grid.permute(1, 2, 0)\n",
        "    plt.figure(figsize=(12,3))\n",
        "    plt.imshow(grid)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT5oBzalqtec",
        "outputId": "c2dd109f-ab90-47dd-8582-2d69f1edfd5e"
      },
      "outputs": [],
      "source": [
        "# Plot the closest images for the first N test images as example\n",
        "for i in range(8):\n",
        "    find_similar_images(test_img_embeds[0][i], test_img_embeds[1][i], key_embeds=train_img_embeds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DICVG6Xwqtec"
      },
      "source": [
        "Based on our autoencoder, we see that we are able to retrieve many similar images to the test input. In particular, in row 4, we can spot that some test images might not be that different from the training set as we thought (same poster, just different scaling/color scaling). We also see that although we haven't given the model any labels, it can cluster different classes in different parts of the latent space (airplane + ship, animals, etc.). This is why autoencoders can also be used as a pre-training strategy for deep networks, especially when we have a large set of unlabeled images (often the case). However, it should be noted that the background still plays a big role in autoencoders while it doesn't for classification. Hence, we don't get \"perfect\" clusters and need to finetune such models for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y78kMcIqtec"
      },
      "source": [
        "### Tensorboard clustering\n",
        "\n",
        "Another way of exploring the similarity of images in the latent space is by dimensionality-reduction methods like PCA or T-SNE. Luckily, Tensorboard provides a nice interface for this and we can make use of it in the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfTCPr2Bqtec"
      },
      "outputs": [],
      "source": [
        "# We use the following model throughout this section.\n",
        "# If you want to try a different latent dimensionality, change it here!\n",
        "trainer = model_dict[128][\"trainer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXVGVTVcqted"
      },
      "outputs": [],
      "source": [
        "# Create a summary writer\n",
        "writer = SummaryWriter(\"tensorboard/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2LWU6qRqted"
      },
      "source": [
        "The function `add_embedding` allows us to add high-dimensional feature vectors to TensorBoard on which we can perform clustering. What we have to provide in the function are the feature vectors, additional metadata such as the labels, and the original images so that we can identify a specific image in the clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9k5OUkGqted"
      },
      "outputs": [],
      "source": [
        "## In case you obtain the following error in the next cell, execute the import statements and last line in this cell\n",
        "##   AttributeError: module 'tensorflow._api.v2.io.gfile' has no attribute 'get_filesystem'\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGAiQuoyqted"
      },
      "outputs": [],
      "source": [
        "# Note: the embedding projector in tensorboard is computationally heavy.\n",
        "# Reduce the image amount below if your computer struggles with visualizing all 10k points\n",
        "NUM_IMGS = len(test_set)\n",
        "\n",
        "writer.add_embedding(test_img_embeds[1][:NUM_IMGS], # Encodings per image\n",
        "                     metadata=[test_set[i][1] for i in range(NUM_IMGS)], # Adding the labels per image to the plot\n",
        "                     label_img=torch.from_numpy(test_img_embeds[0][:NUM_IMGS]+1).permute(0, 3, 1, 2)/2.0) # Adding the original images to the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ9reRCLqted"
      },
      "source": [
        "Finally, we can run tensorboard to explore similarities among images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T-hi8W1qted"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir tensorboard/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJXSO8nLqted"
      },
      "source": [
        "You should be able to see something similar as in the following image. In case the projector stays empty, try to start the TensorBoard outside of the Jupyter notebook.\n",
        "\n",
        "<center><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial9/tensorboard_projector_screenshot.jpeg?raw=1\" width=\"70%\"/></center>\n",
        "\n",
        "Overall, we can see that the model indeed clustered images together that are visually similar. Especially the background color seems to be a crucial factor in the encoding. This correlates to the chosen loss function, here Mean Squared Error on pixel-level because the background is responsible for more than half of the pixels in an average image. Hence, the model learns to focus on it. Nevertheless, we can see that the encodings also separate a couple of classes in the latent space although it hasn't seen any labels. This shows again that autoencoding can also be used as a \"pre-training\"/transfer learning task before classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9yVTzapqted"
      },
      "outputs": [],
      "source": [
        "# Closing the summary writer\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyqhArJzqted"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we have implemented our own autoencoder on small RGB images and explored various properties of the model. In contrast to variational autoencoders, vanilla AEs are not generative and can work on MSE loss functions. This makes them often easier to train. Both versions of AE can be used for dimensionality reduction, as we have seen for finding visually similar images beyond pixel distances. Despite autoencoders gaining less interest in the research community due to their more \"theoretically\" challenging counterpart of VAEs, autoencoders still find usage in a lot of applications like denoising and compression. Hence, AEs are an essential tool that every Deep Learning engineer/researcher should be familiar with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W75J2uCCqted"
      },
      "source": [
        "---\n",
        "\n",
        "[![Star our repository](https://img.shields.io/static/v1.svg?logo=star&label=&message=Star%20Our%20Repository&color=yellow)](https://github.com/phlippe/uvadlc_notebooks/)  If you found this tutorial helpful, consider -ing our repository.    \n",
        "[![Ask questions](https://img.shields.io/static/v1.svg?logo=star&label=&message=Ask%20Questions&color=9cf)](https://github.com/phlippe/uvadlc_notebooks/issues)  For any questions, typos, or bugs that you found, please raise an issue on GitHub.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
