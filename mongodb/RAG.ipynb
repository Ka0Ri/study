{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.embeddings import HuggingFaceHubEmbeddings\n",
    "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
    "# from langchain.document_loaders import DirectoryLoader\n",
    "# from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import gradio as gr\n",
    "from gradio.themes.base import Base\n",
    "# import key_param\n",
    "# from langchain.document_loaders import HuggingFaceDatasetLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the dataset name and the column containing the content\n",
    "# dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "# page_content_column = \"context\"  # or any other column you're interested in\n",
    "\n",
    "# # Create a loader instance\n",
    "# loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)\n",
    "\n",
    "# # Load the data\n",
    "# data = loader.load()\n",
    "\n",
    "# # Display the first 15 entries\n",
    "# data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import DirectoryLoader\n",
    "# from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# loader = DirectoryLoader('./sample_files', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "\n",
    "# documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# # Create an instance of the RecursiveCharacterTextSplitter class with specific parameters.\n",
    "# # It splits text into chunks of 1000 characters each with a 150-character overlap.\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "\n",
    "# # 'data' holds the text you want to split, split the text into documents using the text splitter.\n",
    "# docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\197796\\Anaconda3\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "# embeddings = OpenAIEmbeddings(openai_api_key=key_param.openai_api_key)\n",
    "# hf_token = \"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\"\n",
    "# embeddings = HuggingFaceHubEmbeddings(huggingfacehub_api_token=hf_token)\n",
    "\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", \n",
    "                                                      model_kwargs={\"device\": \"cuda\"})\n",
    "embedding = instructor_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = MongoClient(key_param.MONGO_URI)\n",
    "# dbName = \"langchain\"\n",
    "# collectionName = \"papers\"\n",
    "# collection = client[dbName][collectionName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "\n",
    "## Here is the new embeddings being used\n",
    "\n",
    "\n",
    "# vectordb = MongoDBAtlasVectorSearch.from_documents(documents=docs, \n",
    "#                                     embedding=embedding,\n",
    "#                                     collection=collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seach With MongoDBAtlasVectorSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'vectorSearch',\n",
       " 'fields': [{'path': 'embedding',\n",
       "   'numDimensions': 384,\n",
       "   'similarity': 'cosine',\n",
       "   'type': 'vector'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"type\": \"vectorSearch\",\n",
    "  \"fields\": [{\n",
    "    \"path\": \"embedding\",\n",
    "    \"numDimensions\": 384,\n",
    "    \"similarity\": \"cosine\",\n",
    "    \"type\": \"vector\"\n",
    "  }]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO_URI = \"mongodb+srv://dtvu1707:0@cluster0.mdv28fx.mongodb.net/?retryWrites=true&w=majority\"\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "dbName = \"langchain\"\n",
    "collectionName = \"papers\"\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\"\n",
    "\n",
    "collection = client[dbName][collectionName]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorStore = MongoDBAtlasVectorSearch( collection, embedding, index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'their quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and\\n6.4 points of lift from modeling longer sequences on long-document classiﬁcation [13]. FlashAttention\\nenables the ﬁrst Transformer that can achieve better-than-chance performance on the Path-X [ 80] challenge,\\nsolely from using a longer sequence length (16K). Block-sparse FlashAttention enables a Transformer\\nto scale to even longer sequences (64K), resulting in the ﬁrst model that can achieve better-than-chance\\nperformance on Path-256.\\n•Benchmarking Attention. FlashAttention is up to 3\\x02faster than the standard attention implemen-\\ntation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512,\\nFlashAttention is both faster and more memory-eﬃcient than any existing attention method, whereas\\nfor sequence length beyond 1K, some approximate attention methods (e.g., Linformer) start to become'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is FlashAttention?\"\n",
    "searchDocs = vectorStore.similarity_search(question)\n",
    "searchDocs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "their quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and\n",
      "6.4 points of lift from modeling longer sequences on long-document classiﬁcation [13]. FlashAttention\n",
      "enables the ﬁrst Transformer that can achieve better-than-chance performance on the Path-X [ 80] challenge,\n",
      "solely from using a longer sequence length (16K). Block-sparse FlashAttention enables a Transformer\n",
      "to scale to even longer sequences (64K), resulting in the ﬁrst model that can achieve better-than-chance\n",
      "performance on Path-256.\n",
      "•Benchmarking Attention. FlashAttention is up to 3\u0002faster than the standard attention implemen-\n",
      "tation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512,\n",
      "FlashAttention is both faster and more memory-eﬃcient than any existing attention method, whereas\n",
      "for sequence length beyond 1K, some approximate attention methods (e.g., Linformer) start to become\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorStore.as_retriever()\n",
    "docs = retriever.get_relevant_documents(\"What is FlashAttention?\")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c12b19815d49a0bcd2bcbe02756142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Specify the model name you want to use\n",
    "# model_name = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "\n",
    "# Load the tokenizer associated with the specified model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xl\",\n",
    "                                            #   load_in_8bit=True,\n",
    "                                            #   device_map='auto',\n",
    "                                            #   torch_dtype=torch.float16,\n",
    "                                            #   low_cpu_mem_usage=True,\n",
    "                                            )\n",
    "\n",
    "# Define a question-answering pipeline using the model and tokenizer\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=512,\n",
    "    temperature=0,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")\n",
    "\n",
    "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
    "# with additional model-specific arguments (temperature and max_length)\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipe,\n",
    "    # model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")\n",
    "\n",
    "# Get VectorStoreRetriever: Specifically, Retriever for MongoDB VectorStore.\n",
    "# Implements _get_relevant_documents which retrieves documents relevant to a query.\n",
    "retriever = vectorStore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\197796\\Anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\197796\\Anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toolformer is a software tool for creating a toolbox.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1012 > 512). Running this sequence through the model will result in indexing errors\n",
      "c:\\Users\\197796\\Anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\197796\\Anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm, \n",
    "                                     chain_type=\"stuff\", \n",
    "                                     retriever=retriever,\n",
    "                                     return_source_documents=True)\n",
    "\n",
    "query = \"What is toolformer?\"\n",
    "# Execute the chain\n",
    "print(llm(query))\n",
    "retriever_output = qa(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cite sources\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    reponse = \"\"\n",
    "    reponse += wrap_text_preserve_newlines(llm_response['result'])\n",
    "    reponse += '\\n\\nSources:'\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        reponse += f\"\\n- {source.metadata['source']}\"\n",
    "        \n",
    "    return reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_data(query):\n",
    "    # Convert question to vector using HuggingFace embeddings\n",
    "    # Perform Atlas Vector Search using Langchain's vectorStore\n",
    "    # similarity_search returns MongoDB documents most similar to the query    \n",
    "\n",
    "    docs = vectorStore.similarity_search(query, K=1)\n",
    "    as_output = docs[0].page_content\n",
    "    # Leveraging Atlas Vector Search paired with Langchain's QARetriever\n",
    "\n",
    "    # Load \"stuff\" documents chain. Stuff documents chain takes a list of documents,\n",
    "    # inserts them all into a prompt and passes that prompt to an LLM.\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(llm, \n",
    "                                     chain_type=\"stuff\", \n",
    "                                     retriever=retriever,\n",
    "                                     return_source_documents=True)\n",
    "\n",
    "    # Execute the chain\n",
    "    retriever_output = qa(query)\n",
    "\n",
    "    # Return Atlas Vector Search output, and output generated using RAG Architecture\n",
    "    return as_output, process_llm_response(retriever_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\197796\\Anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\197796\\Anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Measuring massive multitask language understanding. In Advances in Neural Information Processing\\nSystems (NeurIPS) , 2021.\\nNamgyu Ho, Laura Schmid, and Se-Young Yun. Large language mo dels are reasoning teachers, 2022.\\nJordan Hoﬀmann, Sebastian Borgeaud, Arthur Mensch, Elena B uchatskaya, Trevor Cai, Eliza Rutherford,\\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aid an Clark, et al. Training compute-optimal\\nlarge language models. arXiv preprint arXiv:2203.15556 , 2022.\\nJie Huang and Kevin Chen-Chuan Chang. Towards reasoning in l arge language models: A survey. arXiv\\npreprint arXiv:2212.10403 , 2022.\\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Morda tch. Language models as zero-shot planners:\\nExtracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207 , 2022a.\\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, P ete Florence, Andy Zeng, Jonathan Tompson,',\n",
       " 'a growing research trend\\n\\nSources:\\n- sample_files\\\\Augmenting LLMs Survey.pdf\\n- sample_files\\\\Augmenting LLMs Survey.pdf\\n- sample_files\\\\Augmenting LLMs Survey.pdf\\n- sample_files\\\\Augmenting LLMs Survey.pdf')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_data(\"What is Large Language Models?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\197796\\Anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\197796\\Anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\197796\\Anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\197796\\Anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(theme=Base(), title=\"Question Answering App using Vector Search + RAG\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # Question Answering App using Atlas Vector Search + RAG Architecture\n",
    "        \"\"\")\n",
    "    textbox = gr.Textbox(label=\"Enter your Question:\")\n",
    "    with gr.Row():\n",
    "        button = gr.Button(\"Submit\", variant=\"primary\")\n",
    "    with gr.Column():\n",
    "        output1 = gr.Textbox(lines=1, max_lines=10, label=\"Output with just Atlas Vector Search (returns text field as is):\")\n",
    "        output2 = gr.Textbox(lines=1, max_lines=10, label=\"Output generated by chaining Atlas Vector Search to Langchain's RetrieverQA + HuggingFace LLM:\")\n",
    "\n",
    "# Call query_data function upon clicking the Submit button\n",
    "\n",
    "    button.click(query_data, textbox, outputs=[output1, output2])\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
